{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Web Scraping - Flipkart Laptop Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_URL = \"https://www.flipkart.com/search?q=laptops&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "}\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_product_data(page_num):\n",
    "    url = f\"{BASE_URL}&page={page_num}\"\n",
    "    print(f\"Scraping Page {page_num}...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve page {page_num}. Status: {response.status_code}\")\n",
    "            return []\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Container for products\n",
    "        products = soup.find_all('div', class_='jIjQ8S')\n",
    "        if not products:\n",
    "             products = soup.find_all('div', class_='tUxRFH')\n",
    "        if not products:\n",
    "            products = soup.find_all('a', class_='k7wcnx')\n",
    "\n",
    "        page_data = []\n",
    "        \n",
    "        for product in products:\n",
    "            try:\n",
    "                # Name\n",
    "                name_tag = product.find('div', class_='RG5Slk')\n",
    "                if not name_tag:\n",
    "                     name_tag = product.find('div', class_='KzDlHZ')\n",
    "                name = name_tag.text.strip() if name_tag else \"N/A\"\n",
    "                \n",
    "                # Price\n",
    "                price_tag = product.find('div', class_='hZ3P6w')\n",
    "                if not price_tag:\n",
    "                    price_tag = product.find('div', class_='Nx9bqj')\n",
    "                price = price_tag.text.strip() if price_tag else \"N/A\"\n",
    "                \n",
    "                # Rating\n",
    "                rating_tag = product.find('div', class_='MKiFS6')\n",
    "                if not rating_tag:\n",
    "                    rating_tag = product.find('div', class_='XQDdHH')\n",
    "                rating = rating_tag.text.strip() if rating_tag else \"N/A\"\n",
    "                \n",
    "                # Reviews & Ratings Count\n",
    "                stats_container = product.find('span', class_='PvbNMB')\n",
    "                ratings_count = \"0\"\n",
    "                reviews_count = \"0\"\n",
    "                \n",
    "                if stats_container:\n",
    "                    spans = stats_container.find_all('span')\n",
    "                    for sp in spans:\n",
    "                        txt = sp.text.strip()\n",
    "                        if 'Ratings' in txt:\n",
    "                            ratings_count = txt.replace('Ratings', '').strip()\n",
    "                        elif 'Reviews' in txt:\n",
    "                            reviews_count = txt.replace('Reviews', '').strip()\n",
    "                \n",
    "                # Features\n",
    "                features_list = []\n",
    "                ul = product.find('ul', class_='HwRTzP')\n",
    "                if ul:\n",
    "                    features = ul.find_all('li', class_='DTBslk')\n",
    "                    features_list = [f.text.strip() for f in features]\n",
    "                \n",
    "                feature_str = \" | \".join(features_list)\n",
    "                \n",
    "                page_data.append({\n",
    "                    'Product Name': name,\n",
    "                    'Price': price,\n",
    "                    'Rating': rating,\n",
    "                    'Ratings Count': ratings_count,\n",
    "                    'Reviews Count': reviews_count,\n",
    "                    'Features': feature_str\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing product: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return page_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving page {page_num}: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"Scraping function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping demo...\n",
      "Scraping Page 1...\n",
      "Collected 24 items. Total: 24\n",
      "Scraping complete. Collected 24 items for demo.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "all_data = []\n",
    "target_count = 10  # Reduced for notebook playability\n",
    "current_page = 1\n",
    "\n",
    "print(\"Starting scraping demo...\")\n",
    "\n",
    "while len(all_data) < target_count:\n",
    "    data = get_product_data(current_page)\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data found on this page. Stopping.\")\n",
    "        break\n",
    "        \n",
    "    all_data.extend(data)\n",
    "    print(f\"Collected {len(data)} items. Total: {len(all_data)}\")\n",
    "    \n",
    "    current_page += 1\n",
    "    \n",
    "    # Delay to avoid IP ban\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if current_page > 3: \n",
    "        break\n",
    "\n",
    "print(f\"Scraping complete. Collected {len(all_data)} items for demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping save (to preserve full 'laptops.csv' dataset).\n"
     ]
    }
   ],
   "source": [
    "# Save demo data (Optional)\n",
    "# df = pd.DataFrame(all_data)\n",
    "# df.to_csv('laptops_demo.csv', index=False)\n",
    "print(\"Skipping save (to preserve full 'laptops.csv' dataset).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Dataset Preview (from laptops.csv):\n",
      "                                        Product Name    Price  Rating  \\\n",
      "0  ASUS Vivobook Go 15 AMD Ryzen 3 Quad Core 7320...  ₹30,990     4.3   \n",
      "1  Samsung Galaxy Book4 Edge Series Copilot AI-PC...  ₹59,550     4.4   \n",
      "2  Acer Aspire 3 Intel Celeron Dual Core - (8 GB/...  ₹24,699     3.8   \n",
      "3  Samsung Galaxy Book4 Metal Intel Core i7 13th ...  ₹55,570     4.4   \n",
      "4  Samsung Galaxy Book5 AI Metal Intel Core Ultra...  ₹68,390     4.6   \n",
      "\n",
      "  Ratings Count Reviews Count  \\\n",
      "0         1,722           115   \n",
      "1         2,345           207   \n",
      "2         7,961           696   \n",
      "3         4,754           308   \n",
      "4           434            46   \n",
      "\n",
      "                                            Features  \n",
      "0  AMD Ryzen 3 Quad Core Processor | 8 GB LPDDR5 ...  \n",
      "1  Qualcomm Snapdragon X Processor | 16 GB LPDDR5...  \n",
      "2  Intel Celeron Dual Core Processor | 8 GB DDR4 ...  \n",
      "3  Intel Core i7 Processor (13th Gen) | 16 GB LPD...  \n",
      "4  Intel Core Ultra 5 Processor | 16 GB LPDDR5X R...  \n",
      "\n",
      "Dataset shape: (984, 6)\n",
      "Columns: ['Product Name', 'Price', 'Rating', 'Ratings Count', 'Reviews Count', 'Features']\n"
     ]
    }
   ],
   "source": [
    "# Data Preview\n",
    "if os.path.exists('laptops.csv'):\n",
    "    df_preview = pd.read_csv('laptops.csv')\n",
    "    print(\"\\nFull Dataset Preview (from laptops.csv):\")\n",
    "    print(df_preview.head())\n",
    "    print(f\"\\nDataset shape: {df_preview.shape}\")\n",
    "    print(f\"Columns: {list(df_preview.columns)}\")\n",
    "else:\n",
    "    print(\"laptops.csv not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
